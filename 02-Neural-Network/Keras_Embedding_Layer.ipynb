{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Keras_Embedding_Layer.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZofFv6lGEPP",
        "colab_type": "text"
      },
      "source": [
        "Embedding layer in keras can be used for neural networks.\n",
        "\n",
        "Embedding layer is initialized with random weights and will learn an embedding for all of the words in the training dataset.\n",
        "\n",
        "*Usees*\n",
        "*   To learn a word embedding that can be saved and used in another model later.\n",
        "*   As a deep learning model where the embedding is learned along with the model itself.\n",
        "*   To load a pre-trained word embedding model, a type of transfer learning.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N1C7vYzmWhUd",
        "colab_type": "text"
      },
      "source": [
        "The Embedding layer is defined as the first hidden layer of a network. It must specify 3 arguments:\n",
        "\n",
        "It must specify 3 arguments:\n",
        "\n",
        "1.  **input_dim**: This is the size of the vocabulary in the text data. \n",
        "For example, if your data is integer encoded to values between 0-10, then the size of the vocabulary would be 11 words.\n",
        "2.   **output_dim**: This is the size of the vector space in which words will be embedded. It defines the size of the output vectors from this layer for each word. For example, it could be 32 or 100 or even larger. Test different values for your problem.\n",
        "3.   **input_length**: This is the length of input sequences, as you would define for any input layer of a Keras model. For example, if all of your input documents are comprised of 1000 words, this would be 1000."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9wRG-fVlU8fk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "a012358b-5c44-444a-8d5e-05a0dc041885"
      },
      "source": [
        "from numpy import array\n",
        "from tensorflow.keras.preprocessing.text import one_hot\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Embedding\n",
        "# define documents\n",
        "docs = ['Well done!',\n",
        "\t\t'Good work',\n",
        "\t\t'Great effort',\n",
        "\t\t'nice work',\n",
        "\t\t'Excellent!',\n",
        "\t\t'Weak',\n",
        "\t\t'Poor effort!',\n",
        "\t\t'not good',\n",
        "\t\t'poor work',\n",
        "\t\t'Could have done better.']\n",
        "# define class labels\n",
        "labels = array([1,1,1,1,1,0,0,0,0,0])\n",
        "# integer encode the documents\n",
        "vocab_size = 50\n",
        "encoded_docs = [one_hot(d, vocab_size) for d in docs]\n",
        "print(encoded_docs)\n",
        "# pad documents to a max length of 4 words\n",
        "max_length = 4\n",
        "padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
        "print(padded_docs)\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[39, 48], [6, 3], [49, 41], [44, 3], [34], [24], [35, 41], [8, 6], [35, 3], [23, 14, 48, 7]]\n",
            "[[39 48  0  0]\n",
            " [ 6  3  0  0]\n",
            " [49 41  0  0]\n",
            " [44  3  0  0]\n",
            " [34  0  0  0]\n",
            " [24  0  0  0]\n",
            " [35 41  0  0]\n",
            " [ 8  6  0  0]\n",
            " [35  3  0  0]\n",
            " [23 14 48  7]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZsq60rfd1bK",
        "colab_type": "text"
      },
      "source": [
        "The Embedding has a vocabulary of 50 and an input length of 4. We will choose a small embedding space of 8 dimensions.\n",
        "\n",
        "The model is a simple binary classification model. Importantly, the output from the Embedding layer will be 4 vectors of 8 dimensions each, one for each word. We flatten this to a one 32-element vector to pass on to the Dense output layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "piYbsRBJdg5V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define the model\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 8, input_length=max_length))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1, activation='sigmoid'))"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45PkaT2PeMyf",
        "colab_type": "text"
      },
      "source": [
        "The output of the Embedding layer is a 4Ã—8 matrix and this is squashed to a 32-element vector by the Flatten layer.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pt2_KPDfdkli",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "f265bde7-cae4-40ad-9463-8fd588708dff"
      },
      "source": [
        "# compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "# summarize the model\n",
        "print(model.summary())\n",
        "# fit the model\n",
        "model.fit(padded_docs, labels, epochs=50, verbose=0)\n",
        "# evaluate the model\n",
        "loss, accuracy = model.evaluate(padded_docs, labels, verbose=0)\n",
        "print('Accuracy: %f' % (accuracy*100))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 4, 8)              400       \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 33        \n",
            "=================================================================\n",
            "Total params: 433\n",
            "Trainable params: 433\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Accuracy: 89.999998\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}